{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chat Models & Prompt Templates - Comprehensive Guide\n",
    "\n",
    "This notebook provides a deep dive into LangChain's chat models and prompt engineering techniques using Anthropic's Claude.\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to configure and initialize chat models\n",
    "- Advanced prompt engineering patterns\n",
    "- Memory management for conversational AI\n",
    "- Streaming responses for better UX\n",
    "- Error handling and production-ready practices\n",
    "- Real-world applications and use cases\n",
    "\n",
    "## üéØ Prerequisites\n",
    "- Python 3.8+\n",
    "- Anthropic API key\n",
    "- Basic understanding of LLMs and chat interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "### üìñ What This Cell Does:\n",
    "Imports all necessary libraries for working with LangChain and Claude.\n",
    "\n",
    "### üéì Key Learning:\n",
    "- **langchain_anthropic**: Provides the ChatAnthropic interface to Claude models\n",
    "- **langchain_core.messages**: Message types (HumanMessage, SystemMessage, AIMessage) for chat\n",
    "- **langchain_core.prompts**: Tools for creating structured, reusable prompt templates\n",
    "- **langchain_core.runnables**: Chain composition and message history management\n",
    "- **dotenv**: Secure environment variable management for API keys\n",
    "\n",
    "### ‚ö†Ô∏è Best Practice:\n",
    "Always use environment variables for API keys - never hardcode them in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìñ What This Cell Does:\n",
    "Loads API keys from the `.env` file and validates they exist.\n",
    "\n",
    "### üéì Key Learning:\n",
    "- **load_dotenv()**: Reads `.env` file and loads variables into `os.environ`\n",
    "- **ANTHROPIC_API_KEY**: Required for accessing Claude models\n",
    "- **LANGSMITH_API_KEY**: Optional - enables tracing and debugging of LangChain applications\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Load environment variables from `.env` file\n",
    "2. Retrieve API keys from environment\n",
    "3. Validate that the Anthropic API key exists\n",
    "4. Display configuration status (masking sensitive key data)\n",
    "\n",
    "### ‚ö†Ô∏è Troubleshooting:\n",
    "If you get \"ANTHROPIC_API_KEY not found\" error:\n",
    "1. Ensure `.env` file exists in project root\n",
    "2. Verify `.env` contains: `ANTHROPIC_API_KEY=your_key_here`\n",
    "3. Get your API key from: https://console.anthropic.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are loaded\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "langsmith_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "if not anthropic_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found. Please create a .env file with your API key.\")\n",
    "\n",
    "print(\"‚úì Environment variables loaded successfully\")\n",
    "print(f\"‚úì Anthropic API Key: {'*' * 20}{anthropic_key[-4:] if anthropic_key else 'Not set'}\")\n",
    "print(f\"‚úì LangSmith tracking: {'Enabled' if langsmith_key else 'Disabled (optional)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Chat Model Initialization\n",
    "\n",
    "### üìñ What This Cell Does:\n",
    "Creates a reusable function to initialize Claude chat models with custom configurations.\n",
    "\n",
    "### üéì Key Learning:\n",
    "**Model Configuration Parameters:**\n",
    "- **model_name**: Which Claude version to use (e.g., `claude-3-5-sonnet-20241022`)\n",
    "  - Sonnet: Balanced performance and speed\n",
    "  - Opus: Highest intelligence for complex tasks\n",
    "  - Haiku: Fastest, most cost-effective\n",
    "\n",
    "- **temperature** (0.0-1.0): Controls randomness/creativity\n",
    "  - 0.0-0.3: Focused, deterministic (good for factual Q&A, code generation)\n",
    "  - 0.4-0.7: Balanced (default for general chat)\n",
    "  - 0.8-1.0: Creative, varied (good for creative writing, brainstorming)\n",
    "\n",
    "- **max_tokens**: Maximum length of response (up to 4096 for Claude)\n",
    "  - Higher = longer responses but more cost\n",
    "  - Lower = more concise responses, faster, cheaper\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Define factory function for creating chat models\n",
    "2. Set sensible defaults (temperature=0.7, max_tokens=4096)\n",
    "3. Create a default model instance\n",
    "4. Display model configuration for verification\n",
    "\n",
    "### üîç When to Adjust:\n",
    "- **Lower temperature** when you need consistent, accurate answers\n",
    "- **Higher temperature** when you want diverse, creative outputs\n",
    "- **Lower max_tokens** for quick responses or cost optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude model with various configurations\n",
    "def create_chat_model(\n",
    "    model_name: str = \"claude-3-5-sonnet-20241022\",\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 4096\n",
    ") -> ChatAnthropic:\n",
    "    \"\"\"Create and configure a Claude chat model.\"\"\"\n",
    "    return ChatAnthropic(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        anthropic_api_key=anthropic_key\n",
    "    )\n",
    "\n",
    "# Create default model instance\n",
    "chat_model = create_chat_model()\n",
    "print(f\"‚úì Chat model initialized: {chat_model.model}\")\n",
    "print(f\"  - Temperature: {chat_model.temperature}\")\n",
    "print(f\"  - Max tokens: {chat_model.max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Basic Chat Interaction\n",
    "\n",
    "### üìñ What This Cell Does:\n",
    "Demonstrates the simplest way to interact with Claude - sending a single message and getting a response.\n",
    "\n",
    "### üéì Key Learning:\n",
    "- **HumanMessage**: Represents user input in the conversation\n",
    "- **invoke()**: Synchronous method to send messages and wait for complete response\n",
    "- **response.content**: The actual text content of Claude's reply\n",
    "- **Error handling**: Always wrap API calls in try-except blocks\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Create a simple chat function that takes a question string\n",
    "2. Wrap the question in a HumanMessage object\n",
    "3. Send it to Claude using invoke()\n",
    "4. Extract and return the text response\n",
    "5. Handle any errors gracefully\n",
    "\n",
    "### üéØ Use Case:\n",
    "Perfect for:\n",
    "- Single-turn Q&A\n",
    "- Simple queries that don't need context\n",
    "- Quick tests and prototyping\n",
    "\n",
    "### ‚ö° Performance Note:\n",
    "This is synchronous (blocking) - for real applications with long responses, consider streaming (covered later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat interaction\n",
    "def simple_chat(question: str) -> str:\n",
    "    \"\"\"Send a simple question to Claude and get a response.\"\"\"\n",
    "    try:\n",
    "        messages = [HumanMessage(content=question)]\n",
    "        response = chat_model.invoke(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test basic chat\n",
    "question = \"What is LangChain and why is it useful?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "response = simple_chat(question)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Advanced Prompt Engineering\n",
    "\n",
    "### üìñ What This Cell Does:\n",
    "Creates reusable prompt templates for different specialized tasks.\n",
    "\n",
    "### üéì Key Learning - Prompt Templates:\n",
    "**Why use templates?**\n",
    "- **Reusability**: Write once, use with different inputs\n",
    "- **Consistency**: Ensure same structure across multiple calls\n",
    "- **Maintainability**: Change behavior in one place\n",
    "- **Type safety**: Variables are clearly defined\n",
    "\n",
    "**Template Structure:**\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"System instructions with {variable}\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "### üí° Three Template Patterns:\n",
    "\n",
    "**1. Q&A Template (qa_prompt)**\n",
    "- **Purpose**: Factual, accurate responses with expertise\n",
    "- **Variables**: {domain}, {question}\n",
    "- **Best for**: Technical documentation, research, explanations\n",
    "- **Temperature**: Low (0.2-0.4) for accuracy\n",
    "\n",
    "**2. Creative Writing Template (creative_prompt)**\n",
    "- **Purpose**: Generate engaging narratives and content\n",
    "- **Variables**: {genre}, {request}\n",
    "- **Best for**: Stories, marketing copy, creative content\n",
    "- **Temperature**: High (0.7-0.9) for variety\n",
    "\n",
    "**3. Code Generation Template (code_prompt)**\n",
    "- **Purpose**: Generate production-ready code with best practices\n",
    "- **Variables**: {language}, {task}\n",
    "- **Best for**: Code generation, refactoring, examples\n",
    "- **Temperature**: Low (0.2-0.3) for deterministic output\n",
    "\n",
    "### üéØ Task Performed:\n",
    "Define three specialized prompt templates, each with:\n",
    "1. Role definition (system message)\n",
    "2. Behavioral guidelines\n",
    "3. Output expectations\n",
    "4. Variable placeholders for dynamic content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced prompt templates\n",
    "\n",
    "# Template 1: Question Answering with Context\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert AI assistant specializing in {domain}. \n",
    "    Your responses should be:\n",
    "    - Accurate and well-researched\n",
    "    - Clear and concise\n",
    "    - Include examples when appropriate\n",
    "    - Cite sources or reasoning when possible\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Template 2: Creative Writing Assistant\n",
    "creative_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a creative writing assistant with expertise in {genre}.\n",
    "    Help users craft compelling narratives with:\n",
    "    - Rich descriptions and vivid imagery\n",
    "    - Strong character development\n",
    "    - Engaging plot structures\n",
    "    - Appropriate tone and style\"\"\"),\n",
    "    (\"human\", \"{request}\")\n",
    "])\n",
    "\n",
    "# Template 3: Code Generation Assistant\n",
    "code_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert {language} programmer.\n",
    "    When generating code:\n",
    "    - Follow best practices and design patterns\n",
    "    - Include clear comments and documentation\n",
    "    - Consider edge cases and error handling\n",
    "    - Write clean, maintainable, and efficient code\"\"\"),\n",
    "    (\"human\", \"{task}\")\n",
    "])\n",
    "\n",
    "print(\"‚úì Prompt templates created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìñ What This Cell Does:\n",
    "Demonstrates the Q&A template in action with domain expertise.\n",
    "\n",
    "### üéì Key Learning - Chain Composition:\n",
    "**The Pipe Operator `|`:**\n",
    "```python\n",
    "chain = prompt | chat_model | output_parser\n",
    "```\n",
    "\n",
    "This creates a data pipeline:\n",
    "1. **prompt**: Formats the template with variables\n",
    "2. **chat_model**: Sends to Claude and gets response\n",
    "3. **output_parser**: Extracts string content from response object\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Create a chain: `qa_prompt | chat_model | StrOutputParser()`\n",
    "2. Invoke with specific domain (\"machine learning\") and question\n",
    "3. Get a parsed string response\n",
    "\n",
    "### üéØ Example Use Case:\n",
    "Building a technical Q&A chatbot where:\n",
    "- Domain can be: \"cybersecurity\", \"data science\", \"web development\"\n",
    "- Users ask specific technical questions\n",
    "- Responses are authoritative and educational\n",
    "\n",
    "### üìä Expected Output:\n",
    "A comprehensive explanation of supervised vs unsupervised learning with:\n",
    "- Clear definitions\n",
    "- Real-world examples (e.g., email spam filtering, customer segmentation)\n",
    "- Key differences highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Q&A with domain expertise\n",
    "qa_chain = qa_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "qa_response = qa_chain.invoke({\n",
    "    \"domain\": \"machine learning and artificial intelligence\",\n",
    "    \"question\": \"Explain the difference between supervised and unsupervised learning with real-world examples.\"\n",
    "})\n",
    "\n",
    "print(\"=== Q&A Example ===\")\n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìñ What This Cell Does:\n",
    "Uses the creative writing template to generate narrative content.\n",
    "\n",
    "### üéì Key Learning:\n",
    "**Same chain pattern, different purpose:**\n",
    "- Same technical structure (`prompt | model | parser`)\n",
    "- Different system instructions\n",
    "- Different variables ({genre}, {request})\n",
    "- Different output style (creative vs factual)\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Create creative writing chain\n",
    "2. Specify genre (\"science fiction\")\n",
    "3. Request specific creative content\n",
    "4. Get creative, narrative-style response\n",
    "\n",
    "### üéØ Use Cases:\n",
    "- Content marketing (blog posts, product descriptions)\n",
    "- Story generation for games or entertainment\n",
    "- Creative brainstorming\n",
    "- Character development\n",
    "\n",
    "### üìä Expected Output:\n",
    "An engaging opening paragraph with:\n",
    "- Vivid descriptions\n",
    "- Emotional depth\n",
    "- Compelling narrative hook\n",
    "- Genre-appropriate tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate creative writing\n",
    "creative_chain = creative_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "creative_response = creative_chain.invoke({\n",
    "    \"genre\": \"science fiction\",\n",
    "    \"request\": \"Write the opening paragraph of a story about an AI that discovers emotions for the first time.\"\n",
    "})\n",
    "\n",
    "print(\"=== Creative Writing Example ===\")\n",
    "print(creative_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìñ What This Cell Does:\n",
    "Demonstrates code generation with best practices.\n",
    "\n",
    "### üéì Key Learning:\n",
    "**Code Generation Best Practices:**\n",
    "1. **Specify language**: Python, JavaScript, Java, etc.\n",
    "2. **Clear requirements**: What should the code do?\n",
    "3. **Quality expectations**: Error handling, edge cases, documentation\n",
    "4. **Lower temperature**: More deterministic, reliable code\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Create code generation chain\n",
    "2. Specify language (\"Python\")\n",
    "3. Request specific algorithm with requirements\n",
    "4. Get production-ready code with:\n",
    "   - Function definition\n",
    "   - Docstrings\n",
    "   - Type hints\n",
    "   - Error handling\n",
    "   - Example usage\n",
    "\n",
    "### üéØ Use Cases:\n",
    "- Code assistance tools\n",
    "- Learning platforms\n",
    "- Rapid prototyping\n",
    "- Code review and refactoring\n",
    "\n",
    "### üìä Expected Output:\n",
    "Complete binary search implementation with:\n",
    "- Input validation\n",
    "- Edge case handling (empty list, not found)\n",
    "- Clear comments\n",
    "- Time complexity notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate code generation\n",
    "code_chain = code_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "code_response = code_chain.invoke({\n",
    "    \"language\": \"Python\",\n",
    "    \"task\": \"Create a function that implements a binary search algorithm with proper error handling.\"\n",
    "})\n",
    "\n",
    "print(\"=== Code Generation Example ===\")\n",
    "print(code_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Conversational Memory\n",
    "\n",
    "### üìñ What This Cell Does:\n",
    "Sets up conversation memory to enable multi-turn dialogues where the AI remembers previous context.\n",
    "\n",
    "### üéì Key Learning - Memory Management:\n",
    "\n",
    "**Why Memory Matters:**\n",
    "- Enables natural conversations\n",
    "- AI remembers user preferences and previous statements\n",
    "- Allows follow-up questions without repeating context\n",
    "- Creates more helpful, context-aware interactions\n",
    "\n",
    "**Memory Architecture:**\n",
    "```\n",
    "store = {}  # Dictionary to hold all sessions\n",
    "  ‚îî‚îÄ session_id_1\n",
    "       ‚îî‚îÄ InMemoryChatMessageHistory()\n",
    "           ‚îî‚îÄ [HumanMessage, AIMessage, HumanMessage, ...]\n",
    "  ‚îî‚îÄ session_id_2\n",
    "       ‚îî‚îÄ InMemoryChatMessageHistory()\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **store**: Dictionary holding all session histories\n",
    "2. **session_id**: Unique identifier for each conversation\n",
    "3. **InMemoryChatMessageHistory**: Stores message list in memory\n",
    "4. **get_session_history()**: Retrieves or creates history for a session\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Initialize empty store dictionary\n",
    "2. Create function to get/create session history\n",
    "3. Define conversational prompt with MessagesPlaceholder\n",
    "4. Create base chain\n",
    "5. Wrap with RunnableWithMessageHistory for automatic memory\n",
    "\n",
    "### üîç MessagesPlaceholder:\n",
    "- Special placeholder for injecting conversation history\n",
    "- Dynamically inserts previous messages\n",
    "- Must match history_messages_key in RunnableWithMessageHistory\n",
    "\n",
    "### ‚ö†Ô∏è Production Note:\n",
    "This uses in-memory storage (lost when program stops).\n",
    "For production, use:\n",
    "- Database storage (PostgreSQL, MongoDB)\n",
    "- Redis for caching\n",
    "- Cloud storage (AWS DynamoDB, Google Firestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up conversation memory storage\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create conversational prompt with memory\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful AI assistant engaged in a natural conversation.\n",
    "    Remember the context of our conversation and refer back to previous messages when relevant.\n",
    "    Be friendly, informative, and maintain consistency throughout the dialogue.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the conversational chain\n",
    "conversational_chain = conversational_prompt | chat_model\n",
    "\n",
    "# Wrap with message history\n",
    "chat_with_memory = RunnableWithMessageHistory(\n",
    "    conversational_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Conversational chain with memory initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìñ What This Cell Does:\n",
    "Demonstrates a multi-turn conversation where Claude remembers previous context.\n",
    "\n",
    "### üéì Key Learning - Context Awareness:\n",
    "\n",
    "**Conversation Flow:**\n",
    "```\n",
    "Turn 1: User introduces themselves\n",
    "  ‚îî‚îÄ AI: Stores name and interest in Python\n",
    "\n",
    "Turn 2: User asks follow-up (\"resources\")\n",
    "  ‚îî‚îÄ AI: Knows context (Python for beginners)\n",
    "  ‚îî‚îÄ AI: Recommends relevant resources\n",
    "\n",
    "Turn 3: User tests memory (\"my name?\")\n",
    "  ‚îî‚îÄ AI: Recalls \"Alex\" from Turn 1\n",
    "```\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Create helper function for conversational interaction\n",
    "2. Start conversation with introduction (establishes context)\n",
    "3. Ask follow-up question (tests context understanding)\n",
    "4. Test memory recall (validates memory works)\n",
    "\n",
    "### üéØ What Makes This Powerful:\n",
    "- **No context repetition**: User doesn't need to re-explain\n",
    "- **Natural flow**: Like talking to a human\n",
    "- **Contextual responses**: AI tailors answers based on history\n",
    "- **Pronoun resolution**: AI understands \"my\" and \"this\" references\n",
    "\n",
    "### üîç Session Management:\n",
    "```python\n",
    "config={\"configurable\": {\"session_id\": session_id}}\n",
    "```\n",
    "- Each session_id = separate conversation\n",
    "- Enables multiple concurrent users\n",
    "- Isolates conversations from each other\n",
    "\n",
    "### üìä Expected Output:\n",
    "- Turn 1: Friendly greeting, acknowledgment of name and interest\n",
    "- Turn 2: Python learning resources (books, courses, practice sites)\n",
    "- Turn 3: Correctly recalls \"Alex\" from first message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-turn conversation\n",
    "def chat_with_context(message: str, session_id: str = \"default\") -> str:\n",
    "    \"\"\"Send a message and get a response with conversation history.\"\"\"\n",
    "    try:\n",
    "        response = chat_with_memory.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Multi-turn conversation example\n",
    "session_id = \"demo_session_1\"\n",
    "\n",
    "print(\"=== Multi-Turn Conversation Demo ===\")\n",
    "\n",
    "# Turn 1\n",
    "msg1 = \"Hi! My name is Alex and I'm learning Python programming.\"\n",
    "print(f\"\\nUser: {msg1}\")\n",
    "resp1 = chat_with_context(msg1, session_id)\n",
    "print(f\"Assistant: {resp1}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Turn 2\n",
    "msg2 = \"Can you recommend some good resources for beginners?\"\n",
    "print(f\"\\nUser: {msg2}\")\n",
    "resp2 = chat_with_context(msg2, session_id)\n",
    "print(f\"Assistant: {resp2}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Turn 3\n",
    "msg3 = \"What was my name again?\"\n",
    "print(f\"\\nUser: {msg3}\")\n",
    "resp3 = chat_with_context(msg3, session_id)\n",
    "print(f\"Assistant: {resp3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Streaming Responses\n",
    "\n",
    "### üìñ What This Cell Does:\n",
    "Implements streaming to display AI responses in real-time as they're generated.\n",
    "\n",
    "### üéì Key Learning - Streaming vs Invoke:\n",
    "\n",
    "**invoke() - All at Once:**\n",
    "```\n",
    "User sends message ‚Üí [Wait...] ‚Üí Complete response appears\n",
    "```\n",
    "- Simple to implement\n",
    "- User waits for entire response\n",
    "- No feedback during generation\n",
    "\n",
    "**stream() - Token by Token:**\n",
    "```\n",
    "User sends message ‚Üí The ‚Üí response ‚Üí appears ‚Üí word ‚Üí by ‚Üí word\n",
    "```\n",
    "- Better user experience\n",
    "- Immediate feedback\n",
    "- Feels more interactive\n",
    "- Like ChatGPT interface\n",
    "\n",
    "### üí° Task Performed:\n",
    "1. Create function that uses `chat_model.stream()` instead of `invoke()`\n",
    "2. Iterate over response chunks\n",
    "3. Print each chunk immediately with `flush=True`\n",
    "4. Handle errors gracefully\n",
    "\n",
    "### üéØ When to Use Streaming:\n",
    "- ‚úÖ Web chat interfaces\n",
    "- ‚úÖ Long-form content generation\n",
    "- ‚úÖ Interactive applications\n",
    "- ‚ùå Batch processing\n",
    "- ‚ùå When you need complete response first\n",
    "\n",
    "### üîç Technical Details:\n",
    "```python\n",
    "for chunk in chat_model.stream(...):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "```\n",
    "- `end=\"\"`: No newline after each chunk\n",
    "- `flush=True`: Force immediate output\n",
    "- Each chunk contains partial response\n",
    "\n",
    "### üìä Expected Output:\n",
    "Story appears word-by-word in real-time, creating engaging user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate streaming for real-time responses\n",
    "def stream_response(message: str) -> None:\n",
    "    \"\"\"Stream a response token by token.\"\"\"\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    try:\n",
    "        for chunk in chat_model.stream([HumanMessage(content=message)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during streaming: {str(e)}\")\n",
    "\n",
    "print(\"=== Streaming Response Demo ===\")\n",
    "print(\"\\nUser: Tell me a short story about a robot learning to paint.\\n\")\n",
    "stream_response(\"Tell me a short story about a robot learning to paint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Summary\n",
    "\n",
    "### üéâ What You've Learned:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Chat Model Basics**\n",
    "- Initialize Claude models with custom configurations\n",
    "- Understand temperature and token limits\n",
    "- Send simple messages and receive responses\n",
    "\n",
    "#### 2Ô∏è‚É£ **Prompt Engineering**\n",
    "- Create reusable prompt templates\n",
    "- Use system messages to define AI behavior\n",
    "- Build specialized assistants (Q&A, Creative, Code)\n",
    "- Compose chains with pipe operator (|)\n",
    "\n",
    "#### 3Ô∏è‚É£ **Memory & Context**\n",
    "- Implement conversation memory\n",
    "- Enable multi-turn dialogues\n",
    "- Manage multiple concurrent sessions\n",
    "- Use MessagesPlaceholder for history injection\n",
    "\n",
    "#### 4Ô∏è‚É£ **Advanced Features**\n",
    "- Stream responses for better UX\n",
    "- Handle errors gracefully\n",
    "- Parse outputs efficiently\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **RAG (Retrieval-Augmented Generation)**: Add knowledge bases\n",
    "2. **Function Calling**: Enable AI to use tools\n",
    "3. **Multi-Agent Systems**: Multiple specialized AIs working together\n",
    "4. **Production Deployment**: Build web apps with FastAPI/Flask\n",
    "5. **Vector Databases**: Semantic search with embeddings\n",
    "\n",
    "### üìö Resources:\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Anthropic Claude Docs](https://docs.anthropic.com/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### üí° Best Practices Recap:\n",
    "‚úÖ Use environment variables for API keys  \n",
    "‚úÖ Implement error handling and retries  \n",
    "‚úÖ Adjust temperature based on use case  \n",
    "‚úÖ Use templates for reusability  \n",
    "‚úÖ Stream long responses  \n",
    "‚úÖ Implement session management for multi-user apps  \n",
    "‚úÖ Consider persistent storage for production  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== Session Summary ===\")\n",
    "print(f\"\\nActive Sessions: {len(store)}\")\n",
    "print(f\"Session IDs: {list(store.keys())}\")\n",
    "\n",
    "if store:\n",
    "    for session_id, history in store.items():\n",
    "        print(f\"\\n  {session_id}:\")\n",
    "        print(f\"    Messages: {len(history.messages)}\")\n",
    "        print(f\"    User turns: {sum(1 for m in history.messages if isinstance(m, HumanMessage))}\")\n",
    "        print(f\"    AI turns: {sum(1 for m in history.messages if isinstance(m, AIMessage))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
