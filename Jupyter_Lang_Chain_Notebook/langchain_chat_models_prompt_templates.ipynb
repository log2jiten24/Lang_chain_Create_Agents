{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chat Models & Prompt Templates - Comprehensive Guide\n",
    "\n",
    "This notebook provides a deep dive into LangChain's chat models and prompt engineering techniques using Anthropic's Claude.\n",
    "\n",
    "## 📚 Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- How to configure and initialize chat models\n",
    "- Advanced prompt engineering patterns\n",
    "- Memory management for conversational AI\n",
    "- Streaming responses for better UX\n",
    "- Error handling and production-ready practices\n",
    "- Real-world applications and use cases\n",
    "\n",
    "## 🎯 Prerequisites\n",
    "- Python 3.8+\n",
    "- Anthropic API key\n",
    "- Basic understanding of LLMs and chat interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "### 📖 What This Cell Does:\n",
    "Imports all necessary libraries for working with LangChain and Claude.\n",
    "\n",
    "### 🎓 Key Learning:\n",
    "- **langchain_anthropic**: Provides the ChatAnthropic interface to Claude models\n",
    "- **langchain_core.messages**: Message types (HumanMessage, SystemMessage, AIMessage) for chat\n",
    "- **langchain_core.prompts**: Tools for creating structured, reusable prompt templates\n",
    "- **langchain_core.runnables**: Chain composition and message history management\n",
    "- **dotenv**: Secure environment variable management for API keys\n",
    "\n",
    "### ⚠️ Best Practice:\n",
    "Always use environment variables for API keys - never hardcode them in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 📖 What This Cell Does:\n",
    "Loads API keys from the `.env` file and validates they exist.\n",
    "\n",
    "### 🎓 Key Learning:\n",
    "- **load_dotenv()**: Reads `.env` file and loads variables into `os.environ`\n",
    "- **ANTHROPIC_API_KEY**: Required for accessing Claude models\n",
    "- **LANGSMITH_API_KEY**: Optional - enables tracing and debugging of LangChain applications\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Load environment variables from `.env` file\n",
    "2. Retrieve API keys from environment\n",
    "3. Validate that the Anthropic API key exists\n",
    "4. Display configuration status (masking sensitive key data)\n",
    "\n",
    "### ⚠️ Troubleshooting:\n",
    "If you get \"ANTHROPIC_API_KEY not found\" error:\n",
    "1. Ensure `.env` file exists in project root\n",
    "2. Verify `.env` contains: `ANTHROPIC_API_KEY=your_key_here`\n",
    "3. Get your API key from: https://console.anthropic.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are loaded\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "langsmith_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "if not anthropic_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found. Please create a .env file with your API key.\")\n",
    "\n",
    "print(\"✓ Environment variables loaded successfully\")\n",
    "print(f\"✓ Anthropic API Key: {'*' * 20}{anthropic_key[-4:] if anthropic_key else 'Not set'}\")\n",
    "print(f\"✓ LangSmith tracking: {'Enabled' if langsmith_key else 'Disabled (optional)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Chat Model Initialization\n",
    "\n",
    "### 📖 What This Cell Does:\n",
    "Creates a reusable function to initialize Claude chat models with custom configurations.\n",
    "\n",
    "### 🎓 Key Learning:\n",
    "**Model Configuration Parameters:**\n",
    "- **model_name**: Which Claude version to use (e.g., `claude-3-5-sonnet-20241022`)\n",
    "  - Sonnet: Balanced performance and speed\n",
    "  - Opus: Highest intelligence for complex tasks\n",
    "  - Haiku: Fastest, most cost-effective\n",
    "\n",
    "- **temperature** (0.0-1.0): Controls randomness/creativity\n",
    "  - 0.0-0.3: Focused, deterministic (good for factual Q&A, code generation)\n",
    "  - 0.4-0.7: Balanced (default for general chat)\n",
    "  - 0.8-1.0: Creative, varied (good for creative writing, brainstorming)\n",
    "\n",
    "- **max_tokens**: Maximum length of response (up to 4096 for Claude)\n",
    "  - Higher = longer responses but more cost\n",
    "  - Lower = more concise responses, faster, cheaper\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Define factory function for creating chat models\n",
    "2. Set sensible defaults (temperature=0.7, max_tokens=4096)\n",
    "3. Create a default model instance\n",
    "4. Display model configuration for verification\n",
    "\n",
    "### 🔍 When to Adjust:\n",
    "- **Lower temperature** when you need consistent, accurate answers\n",
    "- **Higher temperature** when you want diverse, creative outputs\n",
    "- **Lower max_tokens** for quick responses or cost optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude model with various configurations\n",
    "def create_chat_model(\n",
    "    model_name: str = \"claude-3-5-sonnet-20241022\",\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 4096\n",
    ") -> ChatAnthropic:\n",
    "    \"\"\"Create and configure a Claude chat model.\"\"\"\n",
    "    return ChatAnthropic(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        anthropic_api_key=anthropic_key\n",
    "    )\n",
    "\n",
    "# Create default model instance\n",
    "chat_model = create_chat_model()\n",
    "print(f\"✓ Chat model initialized: {chat_model.model}\")\n",
    "print(f\"  - Temperature: {chat_model.temperature}\")\n",
    "print(f\"  - Max tokens: {chat_model.max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Basic Chat Interaction\n",
    "\n",
    "### 📖 What This Cell Does:\n",
    "Demonstrates the simplest way to interact with Claude - sending a single message and getting a response.\n",
    "\n",
    "### 🎓 Key Learning:\n",
    "- **HumanMessage**: Represents user input in the conversation\n",
    "- **invoke()**: Synchronous method to send messages and wait for complete response\n",
    "- **response.content**: The actual text content of Claude's reply\n",
    "- **Error handling**: Always wrap API calls in try-except blocks\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Create a simple chat function that takes a question string\n",
    "2. Wrap the question in a HumanMessage object\n",
    "3. Send it to Claude using invoke()\n",
    "4. Extract and return the text response\n",
    "5. Handle any errors gracefully\n",
    "\n",
    "### 🎯 Use Case:\n",
    "Perfect for:\n",
    "- Single-turn Q&A\n",
    "- Simple queries that don't need context\n",
    "- Quick tests and prototyping\n",
    "\n",
    "### ⚡ Performance Note:\n",
    "This is synchronous (blocking) - for real applications with long responses, consider streaming (covered later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat interaction\n",
    "def simple_chat(question: str) -> str:\n",
    "    \"\"\"Send a simple question to Claude and get a response.\"\"\"\n",
    "    try:\n",
    "        messages = [HumanMessage(content=question)]\n",
    "        response = chat_model.invoke(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test basic chat\n",
    "question = \"What is LangChain and why is it useful?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "response = simple_chat(question)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Advanced Prompt Engineering\n",
    "\n",
    "### 📖 What This Cell Does:\n",
    "Creates reusable prompt templates for different specialized tasks.\n",
    "\n",
    "### 🎓 Key Learning - Prompt Templates:\n",
    "**Why use templates?**\n",
    "- **Reusability**: Write once, use with different inputs\n",
    "- **Consistency**: Ensure same structure across multiple calls\n",
    "- **Maintainability**: Change behavior in one place\n",
    "- **Type safety**: Variables are clearly defined\n",
    "\n",
    "**Template Structure:**\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"System instructions with {variable}\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "### 💡 Three Template Patterns:\n",
    "\n",
    "**1. Q&A Template (qa_prompt)**\n",
    "- **Purpose**: Factual, accurate responses with expertise\n",
    "- **Variables**: {domain}, {question}\n",
    "- **Best for**: Technical documentation, research, explanations\n",
    "- **Temperature**: Low (0.2-0.4) for accuracy\n",
    "\n",
    "**2. Creative Writing Template (creative_prompt)**\n",
    "- **Purpose**: Generate engaging narratives and content\n",
    "- **Variables**: {genre}, {request}\n",
    "- **Best for**: Stories, marketing copy, creative content\n",
    "- **Temperature**: High (0.7-0.9) for variety\n",
    "\n",
    "**3. Code Generation Template (code_prompt)**\n",
    "- **Purpose**: Generate production-ready code with best practices\n",
    "- **Variables**: {language}, {task}\n",
    "- **Best for**: Code generation, refactoring, examples\n",
    "- **Temperature**: Low (0.2-0.3) for deterministic output\n",
    "\n",
    "### 🎯 Task Performed:\n",
    "Define three specialized prompt templates, each with:\n",
    "1. Role definition (system message)\n",
    "2. Behavioral guidelines\n",
    "3. Output expectations\n",
    "4. Variable placeholders for dynamic content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced prompt templates\n",
    "\n",
    "# Template 1: Question Answering with Context\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert AI assistant specializing in {domain}. \n",
    "    Your responses should be:\n",
    "    - Accurate and well-researched\n",
    "    - Clear and concise\n",
    "    - Include examples when appropriate\n",
    "    - Cite sources or reasoning when possible\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Template 2: Creative Writing Assistant\n",
    "creative_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a creative writing assistant with expertise in {genre}.\n",
    "    Help users craft compelling narratives with:\n",
    "    - Rich descriptions and vivid imagery\n",
    "    - Strong character development\n",
    "    - Engaging plot structures\n",
    "    - Appropriate tone and style\"\"\"),\n",
    "    (\"human\", \"{request}\")\n",
    "])\n",
    "\n",
    "# Template 3: Code Generation Assistant\n",
    "code_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert {language} programmer.\n",
    "    When generating code:\n",
    "    - Follow best practices and design patterns\n",
    "    - Include clear comments and documentation\n",
    "    - Consider edge cases and error handling\n",
    "    - Write clean, maintainable, and efficient code\"\"\"),\n",
    "    (\"human\", \"{task}\")\n",
    "])\n",
    "\n",
    "print(\"✓ Prompt templates created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 📖 What This Cell Does:\n",
    "Demonstrates the Q&A template in action with domain expertise.\n",
    "\n",
    "### 🎓 Key Learning - Chain Composition:\n",
    "**The Pipe Operator `|`:**\n",
    "```python\n",
    "chain = prompt | chat_model | output_parser\n",
    "```\n",
    "\n",
    "This creates a data pipeline:\n",
    "1. **prompt**: Formats the template with variables\n",
    "2. **chat_model**: Sends to Claude and gets response\n",
    "3. **output_parser**: Extracts string content from response object\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Create a chain: `qa_prompt | chat_model | StrOutputParser()`\n",
    "2. Invoke with specific domain (\"machine learning\") and question\n",
    "3. Get a parsed string response\n",
    "\n",
    "### 🎯 Example Use Case:\n",
    "Building a technical Q&A chatbot where:\n",
    "- Domain can be: \"cybersecurity\", \"data science\", \"web development\"\n",
    "- Users ask specific technical questions\n",
    "- Responses are authoritative and educational\n",
    "\n",
    "### 📊 Expected Output:\n",
    "A comprehensive explanation of supervised vs unsupervised learning with:\n",
    "- Clear definitions\n",
    "- Real-world examples (e.g., email spam filtering, customer segmentation)\n",
    "- Key differences highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Q&A with domain expertise\n",
    "qa_chain = qa_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "qa_response = qa_chain.invoke({\n",
    "    \"domain\": \"machine learning and artificial intelligence\",\n",
    "    \"question\": \"Explain the difference between supervised and unsupervised learning with real-world examples.\"\n",
    "})\n",
    "\n",
    "print(\"=== Q&A Example ===\")\n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 📖 What This Cell Does:\n",
    "Uses the creative writing template to generate narrative content.\n",
    "\n",
    "### 🎓 Key Learning:\n",
    "**Same chain pattern, different purpose:**\n",
    "- Same technical structure (`prompt | model | parser`)\n",
    "- Different system instructions\n",
    "- Different variables ({genre}, {request})\n",
    "- Different output style (creative vs factual)\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Create creative writing chain\n",
    "2. Specify genre (\"science fiction\")\n",
    "3. Request specific creative content\n",
    "4. Get creative, narrative-style response\n",
    "\n",
    "### 🎯 Use Cases:\n",
    "- Content marketing (blog posts, product descriptions)\n",
    "- Story generation for games or entertainment\n",
    "- Creative brainstorming\n",
    "- Character development\n",
    "\n",
    "### 📊 Expected Output:\n",
    "An engaging opening paragraph with:\n",
    "- Vivid descriptions\n",
    "- Emotional depth\n",
    "- Compelling narrative hook\n",
    "- Genre-appropriate tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate creative writing\n",
    "creative_chain = creative_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "creative_response = creative_chain.invoke({\n",
    "    \"genre\": \"science fiction\",\n",
    "    \"request\": \"Write the opening paragraph of a story about an AI that discovers emotions for the first time.\"\n",
    "})\n",
    "\n",
    "print(\"=== Creative Writing Example ===\")\n",
    "print(creative_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 📖 What This Cell Does:\n",
    "Demonstrates code generation with best practices.\n",
    "\n",
    "### 🎓 Key Learning:\n",
    "**Code Generation Best Practices:**\n",
    "1. **Specify language**: Python, JavaScript, Java, etc.\n",
    "2. **Clear requirements**: What should the code do?\n",
    "3. **Quality expectations**: Error handling, edge cases, documentation\n",
    "4. **Lower temperature**: More deterministic, reliable code\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Create code generation chain\n",
    "2. Specify language (\"Python\")\n",
    "3. Request specific algorithm with requirements\n",
    "4. Get production-ready code with:\n",
    "   - Function definition\n",
    "   - Docstrings\n",
    "   - Type hints\n",
    "   - Error handling\n",
    "   - Example usage\n",
    "\n",
    "### 🎯 Use Cases:\n",
    "- Code assistance tools\n",
    "- Learning platforms\n",
    "- Rapid prototyping\n",
    "- Code review and refactoring\n",
    "\n",
    "### 📊 Expected Output:\n",
    "Complete binary search implementation with:\n",
    "- Input validation\n",
    "- Edge case handling (empty list, not found)\n",
    "- Clear comments\n",
    "- Time complexity notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate code generation\n",
    "code_chain = code_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "code_response = code_chain.invoke({\n",
    "    \"language\": \"Python\",\n",
    "    \"task\": \"Create a function that implements a binary search algorithm with proper error handling.\"\n",
    "})\n",
    "\n",
    "print(\"=== Code Generation Example ===\")\n",
    "print(code_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Conversational Memory\n",
    "\n",
    "### 📖 What This Cell Does:\n",
    "Sets up conversation memory to enable multi-turn dialogues where the AI remembers previous context.\n",
    "\n",
    "### 🎓 Key Learning - Memory Management:\n",
    "\n",
    "**Why Memory Matters:**\n",
    "- Enables natural conversations\n",
    "- AI remembers user preferences and previous statements\n",
    "- Allows follow-up questions without repeating context\n",
    "- Creates more helpful, context-aware interactions\n",
    "\n",
    "**Memory Architecture:**\n",
    "```\n",
    "store = {}  # Dictionary to hold all sessions\n",
    "  └─ session_id_1\n",
    "       └─ InMemoryChatMessageHistory()\n",
    "           └─ [HumanMessage, AIMessage, HumanMessage, ...]\n",
    "  └─ session_id_2\n",
    "       └─ InMemoryChatMessageHistory()\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **store**: Dictionary holding all session histories\n",
    "2. **session_id**: Unique identifier for each conversation\n",
    "3. **InMemoryChatMessageHistory**: Stores message list in memory\n",
    "4. **get_session_history()**: Retrieves or creates history for a session\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Initialize empty store dictionary\n",
    "2. Create function to get/create session history\n",
    "3. Define conversational prompt with MessagesPlaceholder\n",
    "4. Create base chain\n",
    "5. Wrap with RunnableWithMessageHistory for automatic memory\n",
    "\n",
    "### 🔍 MessagesPlaceholder:\n",
    "- Special placeholder for injecting conversation history\n",
    "- Dynamically inserts previous messages\n",
    "- Must match history_messages_key in RunnableWithMessageHistory\n",
    "\n",
    "### ⚠️ Production Note:\n",
    "This uses in-memory storage (lost when program stops).\n",
    "For production, use:\n",
    "- Database storage (PostgreSQL, MongoDB)\n",
    "- Redis for caching\n",
    "- Cloud storage (AWS DynamoDB, Google Firestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up conversation memory storage\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create conversational prompt with memory\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful AI assistant engaged in a natural conversation.\n",
    "    Remember the context of our conversation and refer back to previous messages when relevant.\n",
    "    Be friendly, informative, and maintain consistency throughout the dialogue.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the conversational chain\n",
    "conversational_chain = conversational_prompt | chat_model\n",
    "\n",
    "# Wrap with message history\n",
    "chat_with_memory = RunnableWithMessageHistory(\n",
    "    conversational_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\"✓ Conversational chain with memory initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 📖 What This Cell Does:\n",
    "Demonstrates a multi-turn conversation where Claude remembers previous context.\n",
    "\n",
    "### 🎓 Key Learning - Context Awareness:\n",
    "\n",
    "**Conversation Flow:**\n",
    "```\n",
    "Turn 1: User introduces themselves\n",
    "  └─ AI: Stores name and interest in Python\n",
    "\n",
    "Turn 2: User asks follow-up (\"resources\")\n",
    "  └─ AI: Knows context (Python for beginners)\n",
    "  └─ AI: Recommends relevant resources\n",
    "\n",
    "Turn 3: User tests memory (\"my name?\")\n",
    "  └─ AI: Recalls \"Alex\" from Turn 1\n",
    "```\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Create helper function for conversational interaction\n",
    "2. Start conversation with introduction (establishes context)\n",
    "3. Ask follow-up question (tests context understanding)\n",
    "4. Test memory recall (validates memory works)\n",
    "\n",
    "### 🎯 What Makes This Powerful:\n",
    "- **No context repetition**: User doesn't need to re-explain\n",
    "- **Natural flow**: Like talking to a human\n",
    "- **Contextual responses**: AI tailors answers based on history\n",
    "- **Pronoun resolution**: AI understands \"my\" and \"this\" references\n",
    "\n",
    "### 🔍 Session Management:\n",
    "```python\n",
    "config={\"configurable\": {\"session_id\": session_id}}\n",
    "```\n",
    "- Each session_id = separate conversation\n",
    "- Enables multiple concurrent users\n",
    "- Isolates conversations from each other\n",
    "\n",
    "### 📊 Expected Output:\n",
    "- Turn 1: Friendly greeting, acknowledgment of name and interest\n",
    "- Turn 2: Python learning resources (books, courses, practice sites)\n",
    "- Turn 3: Correctly recalls \"Alex\" from first message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-turn conversation\n",
    "def chat_with_context(message: str, session_id: str = \"default\") -> str:\n",
    "    \"\"\"Send a message and get a response with conversation history.\"\"\"\n",
    "    try:\n",
    "        response = chat_with_memory.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Multi-turn conversation example\n",
    "session_id = \"demo_session_1\"\n",
    "\n",
    "print(\"=== Multi-Turn Conversation Demo ===\")\n",
    "\n",
    "# Turn 1\n",
    "msg1 = \"Hi! My name is Alex and I'm learning Python programming.\"\n",
    "print(f\"\\nUser: {msg1}\")\n",
    "resp1 = chat_with_context(msg1, session_id)\n",
    "print(f\"Assistant: {resp1}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Turn 2\n",
    "msg2 = \"Can you recommend some good resources for beginners?\"\n",
    "print(f\"\\nUser: {msg2}\")\n",
    "resp2 = chat_with_context(msg2, session_id)\n",
    "print(f\"Assistant: {resp2}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Turn 3\n",
    "msg3 = \"What was my name again?\"\n",
    "print(f\"\\nUser: {msg3}\")\n",
    "resp3 = chat_with_context(msg3, session_id)\n",
    "print(f\"Assistant: {resp3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Streaming Responses\n",
    "\n",
    "### 📖 What This Cell Does:\n",
    "Implements streaming to display AI responses in real-time as they're generated.\n",
    "\n",
    "### 🎓 Key Learning - Streaming vs Invoke:\n",
    "\n",
    "**invoke() - All at Once:**\n",
    "```\n",
    "User sends message → [Wait...] → Complete response appears\n",
    "```\n",
    "- Simple to implement\n",
    "- User waits for entire response\n",
    "- No feedback during generation\n",
    "\n",
    "**stream() - Token by Token:**\n",
    "```\n",
    "User sends message → The → response → appears → word → by → word\n",
    "```\n",
    "- Better user experience\n",
    "- Immediate feedback\n",
    "- Feels more interactive\n",
    "- Like ChatGPT interface\n",
    "\n",
    "### 💡 Task Performed:\n",
    "1. Create function that uses `chat_model.stream()` instead of `invoke()`\n",
    "2. Iterate over response chunks\n",
    "3. Print each chunk immediately with `flush=True`\n",
    "4. Handle errors gracefully\n",
    "\n",
    "### 🎯 When to Use Streaming:\n",
    "- ✅ Web chat interfaces\n",
    "- ✅ Long-form content generation\n",
    "- ✅ Interactive applications\n",
    "- ❌ Batch processing\n",
    "- ❌ When you need complete response first\n",
    "\n",
    "### 🔍 Technical Details:\n",
    "```python\n",
    "for chunk in chat_model.stream(...):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "```\n",
    "- `end=\"\"`: No newline after each chunk\n",
    "- `flush=True`: Force immediate output\n",
    "- Each chunk contains partial response\n",
    "\n",
    "### 📊 Expected Output:\n",
    "Story appears word-by-word in real-time, creating engaging user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate streaming for real-time responses\n",
    "def stream_response(message: str) -> None:\n",
    "    \"\"\"Stream a response token by token.\"\"\"\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    try:\n",
    "        for chunk in chat_model.stream([HumanMessage(content=message)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during streaming: {str(e)}\")\n",
    "\n",
    "print(\"=== Streaming Response Demo ===\")\n",
    "print(\"\\nUser: Tell me a short story about a robot learning to paint.\\n\")\n",
    "stream_response(\"Tell me a short story about a robot learning to paint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Summary\n",
    "\n",
    "### 🎉 What You've Learned:\n",
    "\n",
    "#### 1️⃣ **Chat Model Basics**\n",
    "- Initialize Claude models with custom configurations\n",
    "- Understand temperature and token limits\n",
    "- Send simple messages and receive responses\n",
    "\n",
    "#### 2️⃣ **Prompt Engineering**\n",
    "- Create reusable prompt templates\n",
    "- Use system messages to define AI behavior\n",
    "- Build specialized assistants (Q&A, Creative, Code)\n",
    "- Compose chains with pipe operator (|)\n",
    "\n",
    "#### 3️⃣ **Memory & Context**\n",
    "- Implement conversation memory\n",
    "- Enable multi-turn dialogues\n",
    "- Manage multiple concurrent sessions\n",
    "- Use MessagesPlaceholder for history injection\n",
    "\n",
    "#### 4️⃣ **Advanced Features**\n",
    "- Stream responses for better UX\n",
    "- Handle errors gracefully\n",
    "- Parse outputs efficiently\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. **RAG (Retrieval-Augmented Generation)**: Add knowledge bases\n",
    "2. **Function Calling**: Enable AI to use tools\n",
    "3. **Multi-Agent Systems**: Multiple specialized AIs working together\n",
    "4. **Production Deployment**: Build web apps with FastAPI/Flask\n",
    "5. **Vector Databases**: Semantic search with embeddings\n",
    "\n",
    "### 📚 Resources:\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Anthropic Claude Docs](https://docs.anthropic.com/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### 💡 Best Practices Recap:\n",
    "✅ Use environment variables for API keys  \n",
    "✅ Implement error handling and retries  \n",
    "✅ Adjust temperature based on use case  \n",
    "✅ Use templates for reusability  \n",
    "✅ Stream long responses  \n",
    "✅ Implement session management for multi-user apps  \n",
    "✅ Consider persistent storage for production  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== Session Summary ===\")\n",
    "print(f\"\\nActive Sessions: {len(store)}\")\n",
    "print(f\"Session IDs: {list(store.keys())}\")\n",
    "\n",
    "if store:\n",
    "    for session_id, history in store.items():\n",
    "        print(f\"\\n  {session_id}:\")\n",
    "        print(f\"    Messages: {len(history.messages)}\")\n",
    "        print(f\"    User turns: {sum(1 for m in history.messages if isinstance(m, HumanMessage))}\")\n",
    "        print(f\"    AI turns: {sum(1 for m in history.messages if isinstance(m, AIMessage))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
