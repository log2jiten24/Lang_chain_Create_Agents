{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Chat Application with Claude\n",
    "\n",
    "This notebook demonstrates a sophisticated LLM chat application using LangChain and Anthropic's Claude model.\n",
    "\n",
    "## Features:\n",
    "- Advanced prompt engineering with system and human messages\n",
    "- Conversational chains with memory\n",
    "- Multiple use cases: Q&A, creative writing, code generation\n",
    "- Streaming responses\n",
    "- Error handling\n",
    "- Multi-turn dialogues with context awareness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are loaded\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "langsmith_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "if not anthropic_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found. Please create a .env file with your API key.\")\n",
    "\n",
    "print(\"✓ Environment variables loaded successfully\")\n",
    "print(f\"✓ Anthropic API Key: {'*' * 20}{anthropic_key[-4:] if anthropic_key else 'Not set'}\")\n",
    "print(f\"✓ LangSmith tracking: {'Enabled' if langsmith_key else 'Disabled (optional)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude model with various configurations\n",
    "def create_chat_model(\n",
    "    model_name: str = \"claude-3-5-sonnet-20241022\",\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 4096\n",
    ") -> ChatAnthropic:\n",
    "    \"\"\"Create and configure a Claude chat model.\"\"\"\n",
    "    return ChatAnthropic(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        anthropic_api_key=anthropic_key\n",
    "    )\n",
    "\n",
    "# Create default model instance\n",
    "chat_model = create_chat_model()\n",
    "print(f\"✓ Chat model initialized: {chat_model.model}\")\n",
    "print(f\"  - Temperature: {chat_model.temperature}\")\n",
    "print(f\"  - Max tokens: {chat_model.max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Chat Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat interaction\n",
    "def simple_chat(question: str) -> str:\n",
    "    \"\"\"Send a simple question to Claude and get a response.\"\"\"\n",
    "    try:\n",
    "        messages = [HumanMessage(content=question)]\n",
    "        response = chat_model.invoke(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test basic chat\n",
    "question = \"What is LangChain and why is it useful?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "response = simple_chat(question)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced prompt templates\n",
    "\n",
    "# Template 1: Question Answering with Context\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert AI assistant specializing in {domain}. \n",
    "    Your responses should be:\n",
    "    - Accurate and well-researched\n",
    "    - Clear and concise\n",
    "    - Include examples when appropriate\n",
    "    - Cite sources or reasoning when possible\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Template 2: Creative Writing Assistant\n",
    "creative_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a creative writing assistant with expertise in {genre}.\n",
    "    Help users craft compelling narratives with:\n",
    "    - Rich descriptions and vivid imagery\n",
    "    - Strong character development\n",
    "    - Engaging plot structures\n",
    "    - Appropriate tone and style\"\"\"),\n",
    "    (\"human\", \"{request}\")\n",
    "])\n",
    "\n",
    "# Template 3: Code Generation Assistant\n",
    "code_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert {language} programmer.\n",
    "    When generating code:\n",
    "    - Follow best practices and design patterns\n",
    "    - Include clear comments and documentation\n",
    "    - Consider edge cases and error handling\n",
    "    - Write clean, maintainable, and efficient code\"\"\"),\n",
    "    (\"human\", \"{task}\")\n",
    "])\n",
    "\n",
    "print(\"✓ Prompt templates created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Q&A with domain expertise\n",
    "qa_chain = qa_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "qa_response = qa_chain.invoke({\n",
    "    \"domain\": \"machine learning and artificial intelligence\",\n",
    "    \"question\": \"Explain the difference between supervised and unsupervised learning with real-world examples.\"\n",
    "})\n",
    "\n",
    "print(\"=== Q&A Example ===\\n\")\n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate creative writing\n",
    "creative_chain = creative_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "creative_response = creative_chain.invoke({\n",
    "    \"genre\": \"science fiction\",\n",
    "    \"request\": \"Write the opening paragraph of a story about an AI that discovers emotions for the first time.\"\n",
    "})\n",
    "\n",
    "print(\"=== Creative Writing Example ===\\n\")\n",
    "print(creative_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate code generation\n",
    "code_chain = code_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "code_response = code_chain.invoke({\n",
    "    \"language\": \"Python\",\n",
    "    \"task\": \"Create a function that implements a binary search algorithm with proper error handling.\"\n",
    "})\n",
    "\n",
    "print(\"=== Code Generation Example ===\\n\")\n",
    "print(code_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversational Chain with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up conversation memory storage\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create conversational prompt with memory\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful AI assistant engaged in a natural conversation.\n",
    "    Remember the context of our conversation and refer back to previous messages when relevant.\n",
    "    Be friendly, informative, and maintain consistency throughout the dialogue.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the conversational chain\n",
    "conversational_chain = conversational_prompt | chat_model\n",
    "\n",
    "# Wrap with message history\n",
    "chat_with_memory = RunnableWithMessageHistory(\n",
    "    conversational_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\"✓ Conversational chain with memory initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-turn conversation\n",
    "def chat_with_context(message: str, session_id: str = \"default\") -> str:\n",
    "    \"\"\"Send a message and get a response with conversation history.\"\"\"\n",
    "    try:\n",
    "        response = chat_with_memory.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Multi-turn conversation example\n",
    "session_id = \"demo_session_1\"\n",
    "\n",
    "print(\"=== Multi-Turn Conversation Demo ===\\n\")\n",
    "\n",
    "# Turn 1\n",
    "msg1 = \"Hi! My name is Alex and I'm learning Python programming.\"\n",
    "print(f\"User: {msg1}\")\n",
    "resp1 = chat_with_context(msg1, session_id)\n",
    "print(f\"\\nAssistant: {resp1}\\n\")\n",
    "print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Turn 2\n",
    "msg2 = \"Can you recommend some good resources for beginners?\"\n",
    "print(f\"User: {msg2}\")\n",
    "resp2 = chat_with_context(msg2, session_id)\n",
    "print(f\"\\nAssistant: {resp2}\\n\")\n",
    "print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Turn 3\n",
    "msg3 = \"What was my name again?\"\n",
    "print(f\"User: {msg3}\")\n",
    "resp3 = chat_with_context(msg3, session_id)\n",
    "print(f\"\\nAssistant: {resp3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate streaming for real-time responses\n",
    "def stream_response(message: str) -> None:\n",
    "    \"\"\"Stream a response token by token.\"\"\"\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    try:\n",
    "        for chunk in chat_model.stream([HumanMessage(content=message)]):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during streaming: {str(e)}\")\n",
    "\n",
    "print(\"=== Streaming Response Demo ===\\n\")\n",
    "print(\"User: Tell me a short story about a robot learning to paint.\\n\")\n",
    "stream_response(\"Tell me a short story about a robot learning to paint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Use Case: Context-Aware Research Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a research assistant with context awareness\n",
    "class ResearchAssistant:\n",
    "    \"\"\"An AI research assistant that maintains context across multiple queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, session_id: str = None):\n",
    "        self.session_id = session_id or \"research_session\"\n",
    "        self.chat_model = create_chat_model(temperature=0.3)  # Lower temp for factual responses\n",
    "        \n",
    "        # Create specialized prompt\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert research assistant helping with in-depth analysis.\n",
    "            Your role is to:\n",
    "            1. Provide comprehensive, well-researched answers\n",
    "            2. Break down complex topics into understandable parts\n",
    "            3. Remember previous queries and build upon them\n",
    "            4. Ask clarifying questions when needed\n",
    "            5. Suggest related topics for deeper exploration\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ])\n",
    "        \n",
    "        # Set up chain with memory\n",
    "        chain = self.prompt | self.chat_model\n",
    "        self.chain_with_memory = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"query\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "    \n",
    "    def ask(self, query: str) -> str:\n",
    "        \"\"\"Ask a research question.\"\"\"\n",
    "        try:\n",
    "            response = self.chain_with_memory.invoke(\n",
    "                {\"query\": query},\n",
    "                config={\"configurable\": {\"session_id\": self.session_id}}\n",
    "            )\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def get_history(self) -> List[Any]:\n",
    "        \"\"\"Get conversation history.\"\"\"\n",
    "        if self.session_id in store:\n",
    "            return store[self.session_id].messages\n",
    "        return []\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        if self.session_id in store:\n",
    "            store[self.session_id].clear()\n",
    "\n",
    "print(\"✓ Research Assistant class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the Research Assistant\n",
    "assistant = ResearchAssistant(\"quantum_computing_research\")\n",
    "\n",
    "print(\"=== Research Assistant Demo ===\\n\")\n",
    "\n",
    "# Query 1: Initial question\n",
    "q1 = \"What is quantum computing and how does it differ from classical computing?\"\n",
    "print(f\"Query 1: {q1}\\n\")\n",
    "r1 = assistant.ask(q1)\n",
    "print(f\"Response:\\n{r1}\\n\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Query 2: Follow-up building on context\n",
    "q2 = \"Can you explain more about quantum entanglement mentioned earlier?\"\n",
    "print(f\"Query 2: {q2}\\n\")\n",
    "r2 = assistant.ask(q2)\n",
    "print(f\"Response:\\n{r2}\\n\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Query 3: Application-focused\n",
    "q3 = \"What are some practical applications of this technology?\"\n",
    "print(f\"Query 3: {q3}\\n\")\n",
    "r3 = assistant.ask(q3)\n",
    "print(f\"Response:\\n{r3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate comprehensive error handling\n",
    "class RobustChatInterface:\n",
    "    \"\"\"A robust chat interface with comprehensive error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3):\n",
    "        self.max_retries = max_retries\n",
    "        self.chat_model = create_chat_model()\n",
    "    \n",
    "    def send_message(self, message: str, retry_count: int = 0) -> Dict[str, Any]:\n",
    "        \"\"\"Send a message with retry logic and error handling.\"\"\"\n",
    "        result = {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"error\": None,\n",
    "            \"retry_count\": retry_count\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Input validation\n",
    "            if not message or not message.strip():\n",
    "                raise ValueError(\"Message cannot be empty\")\n",
    "            \n",
    "            if len(message) > 100000:\n",
    "                raise ValueError(\"Message too long (max 100,000 characters)\")\n",
    "            \n",
    "            # Send message\n",
    "            response = self.chat_model.invoke([HumanMessage(content=message)])\n",
    "            result[\"success\"] = True\n",
    "            result[\"response\"] = response.content\n",
    "            \n",
    "        except ValueError as e:\n",
    "            result[\"error\"] = f\"Validation error: {str(e)}\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Check if we should retry\n",
    "            if retry_count < self.max_retries and \"rate limit\" in error_msg.lower():\n",
    "                print(f\"Rate limit hit, retrying... (attempt {retry_count + 1}/{self.max_retries})\")\n",
    "                import time\n",
    "                time.sleep(2 ** retry_count)  # Exponential backoff\n",
    "                return self.send_message(message, retry_count + 1)\n",
    "            \n",
    "            result[\"error\"] = f\"API error: {error_msg}\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test error handling\n",
    "robust_chat = RobustChatInterface()\n",
    "\n",
    "print(\"=== Error Handling Demo ===\\n\")\n",
    "\n",
    "# Test 1: Valid message\n",
    "test1 = robust_chat.send_message(\"What is the capital of France?\")\n",
    "print(f\"Test 1 (Valid): Success={test1['success']}\")\n",
    "if test1['success']:\n",
    "    print(f\"Response: {test1['response'][:100]}...\\n\")\n",
    "\n",
    "# Test 2: Empty message\n",
    "test2 = robust_chat.send_message(\"\")\n",
    "print(f\"Test 2 (Empty): Success={test2['success']}\")\n",
    "if not test2['success']:\n",
    "    print(f\"Error: {test2['error']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practical Use Cases Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 1: Technical Documentation Generator\n",
    "def generate_documentation(code_snippet: str, language: str) -> str:\n",
    "    \"\"\"Generate comprehensive documentation for code.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a technical documentation expert.\n",
    "        Generate clear, comprehensive documentation including:\n",
    "        - Function/class purpose and overview\n",
    "        - Parameter descriptions with types\n",
    "        - Return value description\n",
    "        - Usage examples\n",
    "        - Edge cases and error handling notes\"\"\"),\n",
    "        (\"human\", \"Generate documentation for this {language} code:\\n\\n{code}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | chat_model | StrOutputParser()\n",
    "    return chain.invoke({\"language\": language, \"code\": code_snippet})\n",
    "\n",
    "# Example code\n",
    "sample_code = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Technical Documentation Generator ===\\n\")\n",
    "docs = generate_documentation(sample_code, \"Python\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2: Code Review Assistant\n",
    "def review_code(code: str, language: str, focus_areas: List[str] = None) -> str:\n",
    "    \"\"\"Perform comprehensive code review.\"\"\"\n",
    "    focus = \", \".join(focus_areas) if focus_areas else \"all aspects\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are an expert code reviewer.\n",
    "        Review the code focusing on: {focus}\n",
    "        Provide:\n",
    "        - Overall assessment\n",
    "        - Specific issues with line references\n",
    "        - Security concerns\n",
    "        - Performance considerations\n",
    "        - Suggestions for improvement\n",
    "        - Best practice recommendations\"\"\"),\n",
    "        (\"human\", \"Review this {{language}} code:\\n\\n{{code}}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | chat_model | StrOutputParser()\n",
    "    return chain.invoke({\"language\": language, \"code\": code})\n",
    "\n",
    "review_code_sample = \"\"\"\n",
    "def process_user_data(data):\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item['age'] > 18:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Code Review Assistant ===\\n\")\n",
    "review = review_code(review_code_sample, \"Python\", [\"error handling\", \"code quality\", \"performance\"])\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3: Learning Assistant with Socratic Method\n",
    "class SocraticTutor:\n",
    "    \"\"\"A tutor that uses the Socratic method to guide learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, subject: str, session_id: str = None):\n",
    "        self.subject = subject\n",
    "        self.session_id = session_id or f\"tutor_{subject}\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are a Socratic tutor teaching {subject}.\n",
    "            Instead of giving direct answers:\n",
    "            - Ask guiding questions that lead students to discover answers\n",
    "            - Encourage critical thinking\n",
    "            - Provide hints when students are stuck\n",
    "            - Celebrate insights and correct understanding\n",
    "            - Build on previous exchanges in the conversation\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{student_input}\")\n",
    "        ])\n",
    "        \n",
    "        chain = self.prompt | chat_model\n",
    "        self.tutor_chain = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"student_input\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "    \n",
    "    def teach(self, student_input: str) -> str:\n",
    "        \"\"\"Engage in Socratic dialogue with the student.\"\"\"\n",
    "        response = self.tutor_chain.invoke(\n",
    "            {\"student_input\": student_input},\n",
    "            config={\"configurable\": {\"session_id\": self.session_id}}\n",
    "        )\n",
    "        return response.content\n",
    "\n",
    "# Demonstrate Socratic tutoring\n",
    "tutor = SocraticTutor(\"Python programming\")\n",
    "\n",
    "print(\"\\n=== Socratic Learning Assistant ===\\n\")\n",
    "\n",
    "student1 = \"I want to learn about list comprehensions in Python.\"\n",
    "print(f\"Student: {student1}\\n\")\n",
    "teacher1 = tutor.teach(student1)\n",
    "print(f\"Tutor: {teacher1}\\n\")\n",
    "print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "student2 = \"It's a way to create lists more efficiently?\"\n",
    "print(f\"Student: {student2}\\n\")\n",
    "teacher2 = tutor.teach(student2)\n",
    "print(f\"Tutor: {teacher2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conversation Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze conversation history\n",
    "def analyze_conversation(session_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze conversation history and provide insights.\"\"\"\n",
    "    if session_id not in store:\n",
    "        return {\"error\": \"Session not found\"}\n",
    "    \n",
    "    messages = store[session_id].messages\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_messages\": len(messages),\n",
    "        \"user_messages\": sum(1 for m in messages if isinstance(m, HumanMessage)),\n",
    "        \"assistant_messages\": sum(1 for m in messages if isinstance(m, AIMessage)),\n",
    "        \"average_user_length\": 0,\n",
    "        \"average_assistant_length\": 0,\n",
    "        \"topics\": []\n",
    "    }\n",
    "    \n",
    "    user_lengths = [len(m.content) for m in messages if isinstance(m, HumanMessage)]\n",
    "    assistant_lengths = [len(m.content) for m in messages if isinstance(m, AIMessage)]\n",
    "    \n",
    "    if user_lengths:\n",
    "        analysis[\"average_user_length\"] = sum(user_lengths) / len(user_lengths)\n",
    "    if assistant_lengths:\n",
    "        analysis[\"average_assistant_length\"] = sum(assistant_lengths) / len(assistant_lengths)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze a session\n",
    "print(\"=== Conversation Analysis ===\\n\")\n",
    "if \"demo_session_1\" in store:\n",
    "    stats = analyze_conversation(\"demo_session_1\")\n",
    "    print(f\"Session: demo_session_1\")\n",
    "    print(f\"Total messages: {stats['total_messages']}\")\n",
    "    print(f\"User messages: {stats['user_messages']}\")\n",
    "    print(f\"Assistant messages: {stats['assistant_messages']}\")\n",
    "    print(f\"Avg user message length: {stats['average_user_length']:.0f} chars\")\n",
    "    print(f\"Avg assistant message length: {stats['average_assistant_length']:.0f} chars\")\n",
    "else:\n",
    "    print(\"No conversation history found. Run the multi-turn conversation demo first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive chat interface\n",
    "def interactive_chat_session():\n",
    "    \"\"\"Run an interactive chat session (for notebook use).\"\"\"\n",
    "    print(\"=== Interactive Chat Session ===\")\n",
    "    print(\"Type 'quit' or 'exit' to end the session\")\n",
    "    print(\"Type 'clear' to clear conversation history\")\n",
    "    print(\"Type 'history' to view conversation stats\\n\")\n",
    "    \n",
    "    session_id = \"interactive_session\"\n",
    "    \n",
    "    # Note: In a Jupyter notebook, you would use input() for interactive use\n",
    "    # For demo purposes, we'll show the structure\n",
    "    \n",
    "    example_inputs = [\n",
    "        \"Hello! What can you help me with?\",\n",
    "        \"Tell me about Python decorators\",\n",
    "        \"Can you give me an example?\",\n",
    "        \"history\",\n",
    "        \"quit\"\n",
    "    ]\n",
    "    \n",
    "    for user_input in example_inputs:\n",
    "        print(f\"\\nYou: {user_input}\")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        \n",
    "        elif user_input.lower() == 'clear':\n",
    "            if session_id in store:\n",
    "                store[session_id].clear()\n",
    "            print(\"Conversation history cleared.\")\n",
    "            continue\n",
    "        \n",
    "        elif user_input.lower() == 'history':\n",
    "            stats = analyze_conversation(session_id)\n",
    "            if \"error\" not in stats:\n",
    "                print(f\"\\nConversation Stats:\")\n",
    "                print(f\"  Total messages: {stats['total_messages']}\")\n",
    "                print(f\"  Exchanges: {stats['user_messages']}\")\n",
    "            else:\n",
    "                print(\"No conversation history yet.\")\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        response = chat_with_context(user_input, session_id)\n",
    "        print(f\"\\nAssistant: {response}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Run demo\n",
    "interactive_chat_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we've covered\n",
    "summary = \"\"\"\n",
    "=== LangChain Chat Application Summary ===\n",
    "\n",
    "✓ Core Concepts Covered:\n",
    "  1. Environment setup and API key management\n",
    "  2. Claude model initialization and configuration\n",
    "  3. Basic and advanced prompt engineering\n",
    "  4. Conversational chains with memory\n",
    "  5. Streaming responses\n",
    "  6. Error handling and retry logic\n",
    "  7. Multi-turn conversations with context\n",
    "  8. Specialized assistants (Research, Socratic Tutor)\n",
    "  9. Practical use cases (Documentation, Code Review)\n",
    "  10. Conversation analysis and insights\n",
    "\n",
    "✓ Best Practices:\n",
    "  • Use environment variables for API keys (never hardcode)\n",
    "  • Implement proper error handling and retries\n",
    "  • Adjust temperature based on use case (lower for factual, higher for creative)\n",
    "  • Leverage conversation memory for context-aware interactions\n",
    "  • Use system prompts to define assistant behavior and constraints\n",
    "  • Stream responses for better user experience with long outputs\n",
    "  • Validate inputs before sending to the API\n",
    "  • Monitor and analyze conversations for insights\n",
    "  • Use specialized prompt templates for different tasks\n",
    "  • Implement session management for multiple concurrent conversations\n",
    "\n",
    "✓ Key LangChain Components Used:\n",
    "  • ChatAnthropic: Claude model interface\n",
    "  • ChatPromptTemplate: Structured prompt creation\n",
    "  • MessagesPlaceholder: Dynamic message injection\n",
    "  • RunnableWithMessageHistory: Conversation memory\n",
    "  • InMemoryChatMessageHistory: Session storage\n",
    "  • StrOutputParser: Response parsing\n",
    "  • Chain composition with | operator\n",
    "\n",
    "✓ Next Steps:\n",
    "  • Explore RAG (Retrieval-Augmented Generation) for knowledge bases\n",
    "  • Implement persistent storage (database) for conversation history\n",
    "  • Add vector embeddings for semantic search\n",
    "  • Create custom tools and function calling\n",
    "  • Build multi-agent systems\n",
    "  • Deploy as a web application or API\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for common tasks\n",
    "\n",
    "def export_conversation(session_id: str, filename: str) -> bool:\n",
    "    \"\"\"Export conversation history to a text file.\"\"\"\n",
    "    try:\n",
    "        if session_id not in store:\n",
    "            print(f\"Session '{session_id}' not found.\")\n",
    "            return False\n",
    "        \n",
    "        messages = store[session_id].messages\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Conversation Export - Session: {session_id}\\n\")\n",
    "            f.write(f\"Generated: {__import__('datetime').datetime.now()}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for i, msg in enumerate(messages, 1):\n",
    "                role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "                f.write(f\"[{i}] {role}:\\n{msg.content}\\n\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        print(f\"✓ Conversation exported to {filename}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting conversation: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def list_active_sessions() -> List[str]:\n",
    "    \"\"\"List all active chat sessions.\"\"\"\n",
    "    return list(store.keys())\n",
    "\n",
    "def clear_all_sessions():\n",
    "    \"\"\"Clear all conversation histories.\"\"\"\n",
    "    store.clear()\n",
    "    print(\"✓ All conversation histories cleared.\")\n",
    "\n",
    "print(\"✓ Utility functions loaded\")\n",
    "print(\"\\nAvailable utilities:\")\n",
    "print(\"  • export_conversation(session_id, filename)\")\n",
    "print(\"  • list_active_sessions()\")\n",
    "print(\"  • clear_all_sessions()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
